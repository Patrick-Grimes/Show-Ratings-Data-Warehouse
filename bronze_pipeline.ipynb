{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "909b85cd-7532-4bb5-a53f-3f1860960a76",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os #working with files and directories\n",
    "\n",
    "raw_data_dir = \"/Volumes/anime_warehouse/default/raw_files/raw_data\"\n",
    "\n",
    "# All files in raw_data_dir\n",
    "files = dbutils.fs.ls(raw_data_dir)\n",
    "\n",
    "for file_info in files:\n",
    "    file_name = file_info.name\n",
    "    \n",
    "    # Process only CSV files and skip folders that already exist\n",
    "    if file_name.endswith(\".csv\"):\n",
    "        # Create a folder name name / path\n",
    "        folder_name = file_name.replace(\".csv\", \"\")\n",
    "        folder_path = f\"{raw_data_dir}/{folder_name}\"\n",
    "        \n",
    "        # 2. Create the new sub-folder\n",
    "        dbutils.fs.mkdirs(folder_path)\n",
    "        \n",
    "        # 3. Move the file into it\n",
    "        source_path = f\"{raw_data_dir}/{file_name}\"\n",
    "        destination_path = f\"{folder_path}/{file_name}\"\n",
    "        \n",
    "        dbutils.fs.mv(source_path, destination_path)\n",
    "        print(f\"Moved {file_name} into {folder_path}\")\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ed0c5c55-0ada-4d1d-8a9d-615af25f2b90",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "* Wanted to automate the process of making a new folder for each new csv loaded into my raw_data folder, such that when I run the auto loader, it wont treat all the CSVs as one dataset and lump them together\n",
    "  * also, if i upload a new file into the anime folder, auto loader only processes those new rows and won't get confused by files belonging to other tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2986842b-7b11-4900-b905-f65079895e4d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "table_name = \"anime_warehouse.bronze.raw_anime\"\n",
    "source_path = \"/Volumes/anime_warehouse/default/raw_files/raw_data/\"\n",
    "schema_location = \"/Volumes/anime_warehouse/default/raw_files/checkpoints/raw_anime\"\n",
    "\n",
    "def ingest_bronze(source_path, table_name, schema_location):\n",
    "    \"\"\"\n",
    "    Ingests raw CSVs into a Delta Bronze table using Auto Loader.\n",
    "    \n",
    "    Args:\n",
    "        source_path (str): Path to the raw CSV files\n",
    "        table_name (str): Target table name\n",
    "        schema_location (str): Path to store the schema inference checkpoints\n",
    "    \"\"\"\n",
    "    (spark.readStream\n",
    "        .format(\"cloudFiles\") \n",
    "        .option(\"cloudFiles.format\", \"csv\")\n",
    "        .option(\"cloudFiles.schemaLocation\", schema_location)\n",
    "        .option(\"cloudFiles.inferColumnTypes\", \"true\") \n",
    "        .option(\"header\", \"true\")\n",
    "        .option(\"escape\", '\"')\n",
    "        .load(source_path)\n",
    "        .writeStream\n",
    "        .option(\"checkpointLocation\", f\"{schema_location}/_checkpoint\")\n",
    "        .trigger(availableNow=True) # Process all files once, then stop (Batch mode)\n",
    "        .toTable(table_name)\n",
    "    )\n",
    "    print(f\"Successfully ingested {table_name}\")\n",
    "\n",
    "raw_data_base = \"/Volumes/anime_warehouse/default/raw_files/raw_data\"\n",
    "checkpoint_base = \"/Volumes/anime_warehouse/default/raw_files/checkpoints\"\n",
    "folders = dbutils.fs.ls(raw_data_base)\n",
    "\n",
    "for f in folders:\n",
    "    if f.isDir():\n",
    "        folder_name = f.name.replace(\"/\",\"\")\n",
    "        current_source_path = f.path\n",
    "        current_table_name = f\"anime_warehouse.bronze.raw_{folder_name}\"\n",
    "        current_schema_loc = f\"{checkpoint_base}/raw_{folder_name}\"\n",
    "\n",
    "        ingest_bronze(\n",
    "            source_path = current_source_path,\n",
    "            table_name = current_table_name,\n",
    "            schema_location = current_schema_loc\n",
    "        )\n",
    "\n",
    "print(\"Done\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cc1163fb-fe14-44a8-91f1-432647904936",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "* The script for the actual ingestion process after sorting each CSV into a folder\n",
    "  * Cloud files: automatically detects new files, such that if I rerun, it only picks up new files\n",
    "  * Trigger(availableNow=True): One-time run, batch job\n",
    "  * Schema location: Data lineage, if a new file comes with an extra column, the spark job would normally fail to maintain integrity, but with cloud schema location it keeps a record of the past data and updates it\n",
    "  * toTable() automatically makes the table per csv as a delta table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b620ce64-6023-4d25-b71c-9259fad011df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * FROM anime_warehouse.bronze.raw_anime_genres LIMIT 20;"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8094820192522197,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "bronze_pipeline",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}