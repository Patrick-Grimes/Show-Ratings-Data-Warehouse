{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "899bb611-4f6e-4ef1-941e-11d4b45addcd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, split, explode, to_date, trim, regexp_replace, when"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "434e6bb9-631b-4594-a36c-a2e97a976023",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "* Making sure everything is set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e0f2775-e09c-4c1b-bf98-bea0700f578e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dim_anime\n"
     ]
    }
   ],
   "source": [
    "print(\"dim_anime\")\n",
    "\n",
    "raw_anime_df = spark.read.table(\"anime_warehouse.bronze.raw_anime\")\n",
    "\n",
    "silver_anime_df = (raw_anime_df\n",
    "    .select(\n",
    "        col(\"anime_id\").cast(\"int\"),\n",
    "        trim(col(\"title\")).alias(\"title\"),\n",
    "        col(\"score\").cast(\"double\"),\n",
    "        col(\"rank\").cast(\"int\"),\n",
    "        col(\"popularity\").cast(\"int\"),\n",
    "        col(\"members\").cast(\"long\"),\n",
    "        col(\"synopsis\"),\n",
    "        to_date(col(\"start_date\"), \"yyyy-MM-dd\").alias(\"start_date\"),\n",
    "        to_date(col(\"end_date\"), \"yyyy-MM-dd\").alias(\"end_date\"),\n",
    "        col(\"type\"),\n",
    "        col(\"episodes\").cast(\"int\"),\n",
    "        col(\"image_url\")\n",
    "    )\n",
    "    .dropDuplicates([\"anime_id\"])\n",
    ")\n",
    "\n",
    "silver_anime_df.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(\"anime_warehouse.silver.dim_anime\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06f0b454-b70b-41e6-ae25-4883b5cc3805",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dim_genres\n"
     ]
    }
   ],
   "source": [
    "print(\"dim_genres\")\n",
    "\n",
    "raw_genres_df = spark.read.table(\"anime_warehouse.bronze.raw_anime_genres\")\n",
    "\n",
    "# Fix stuttering \n",
    "# Clean up columns\n",
    "silver_genres_df = (raw_genres_df\n",
    "    .withColumn(\"genre_clean\", regexp_replace(col(\"genre\"), r\"^(.+)\\s+\\1$\", \"$1\"))\n",
    "    .withColumn(\"genre_clean\", regexp_replace(col(\"genre_clean\"), \"Theme::|Demographic::\", \"\"))\n",
    "    .select(\n",
    "        col(\"anime_id\").cast(\"int\"),\n",
    "        trim(col(\"genre_clean\")).alias(\"genre\")\n",
    "    )\n",
    "    .dropDuplicates()\n",
    ")\n",
    "\n",
    "# Save to Silver\n",
    "silver_genres_df.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(\"anime_warehouse.silver.dim_genres\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9618749d-d8a5-4fe1-8872-91d21c9eb1d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dim_staff\n"
     ]
    }
   ],
   "source": [
    "print(\"dim_staff\")\n",
    "\n",
    "raw_staff_df = spark.read.table(\"anime_warehouse.bronze.raw_anime_staff\")\n",
    "\n",
    "# Explode comma-separated roles into individual rows\n",
    "silver_staff_df = (raw_staff_df\n",
    "    .withColumn(\"role_array\", split(col(\"role\"), \", \"))\n",
    "    .withColumn(\"role_exploded\", explode(col(\"role_array\")))\n",
    "    .select(\n",
    "        col(\"anime_id\").cast(\"int\"),\n",
    "        col(\"person_id\").cast(\"int\"),\n",
    "        trim(col(\"role_exploded\")).alias(\"role\")\n",
    "    )\n",
    "    .dropDuplicates()\n",
    ")\n",
    "\n",
    "# Save to Silver\n",
    "silver_staff_df.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(\"anime_warehouse.silver.dim_staff\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "776cecfb-2e11-4465-826f-b1cabc94c940",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dim_entities\n"
     ]
    }
   ],
   "source": [
    "print(\"dim_entities\")\n",
    "\n",
    "raw_entities_df = spark.read.table(\"anime_warehouse.bronze.raw_entities\")\n",
    "\n",
    "# Clean names and types\n",
    "silver_entities_df = (raw_entities_df\n",
    "    .select(\n",
    "        col(\"entity_id\").cast(\"int\"),\n",
    "        col(\"entity_type\"), # character, voice_actor, studio, etc\n",
    "        trim(col(\"name\")).alias(\"name\"),\n",
    "        col(\"image_url\")\n",
    "    )\n",
    "    .dropDuplicates([\"entity_id\"])\n",
    ")\n",
    "\n",
    "# Save to Silver\n",
    "silver_entities_df.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(\"anime_warehouse.silver.dim_entities\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "31c16b53-993d-440e-a208-7cc077f4ae5b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relationship Bridges\nAll Silver transformations done\n"
     ]
    }
   ],
   "source": [
    "print(\"Relationship Bridges\")\n",
    "\n",
    "# Voice Actors Bridge\n",
    "spark.read.table(\"anime_warehouse.bronze.raw_anime_voice_actors\") \\\n",
    "    .select(\n",
    "        col(\"character_id\").cast(\"int\"), \n",
    "        col(\"person_id\").cast(\"int\"), \n",
    "        trim(col(\"language\")).alias(\"language\")\n",
    "    ) \\\n",
    "    .dropDuplicates() \\\n",
    "    .write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"anime_warehouse.silver.bridge_voice_actors\")\n",
    "\n",
    "# Company Relationship Bridge\n",
    "spark.read.table(\"anime_warehouse.bronze.raw_anime_companies\") \\\n",
    "    .select(\n",
    "        col(\"anime_id\").cast(\"int\"), \n",
    "        col(\"company_id\").cast(\"int\"), \n",
    "        trim(col(\"role\")).alias(\"role\")\n",
    "    ) \\\n",
    "    .dropDuplicates() \\\n",
    "    .write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"anime_warehouse.silver.bridge_anime_companies\")\n",
    "\n",
    "# Character Roles Bridge\n",
    "spark.read.table(\"anime_warehouse.bronze.raw_anime_characters\") \\\n",
    "    .select(\n",
    "        col(\"anime_id\").cast(\"int\"), \n",
    "        col(\"character_id\").cast(\"int\"), \n",
    "        trim(col(\"role\")).alias(\"role\")\n",
    "    ) \\\n",
    "    .dropDuplicates() \\\n",
    "    .write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"anime_warehouse.silver.bridge_anime_characters\")\n",
    "\n",
    "print(\"All Silver transformations done\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "silver transformations",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}